---
layout: post
title: Understanding GANs through a statistical divergence perspective
published: true
---

Most introductions to Generative adversarial networks (GAN) focus on a game-theoretic perspective with two players where one player (the discriminator) tries to differentiate the real data from the fake data while the other player (the generator) learns to generate realistic data by trying to "fool" the discriminator. However, this view of GANs is very limited as it does not generalize to GANs other than the first one formulated by [Goodfellow et al. (2014)](https://arxiv.org/abs/1406.2661) and a few specific cases such as one specific choice of the [Least Squares GAN](https://arxiv.org/abs/1611.04076). In this blog post, I'm going to explain how GANs can be understood as a divergence minimization rather than an adversarial game.

# Problem at hand

Let's start from a simple problem, let say we want to minimize the distance between an object in $\mathbb{R}^n$ and an object generated by a neural network (assuming a fixed input so that the neural network always generate the same thing). The solution is to minimize some sort of distance between the two objects until we reach convergence. A slightly harder problem would be to minimize the distance between a set of objects and another set of objects generated by a neural network, however, this would make the assumption that all our real data is the only data possible (exhausting cases). In GANs we assume that the real data come from an underlying probability distribution $P$ and that the data generated by the generator comes from an underlying probability distribution $Q_{\theta}$ (produced by a neural network); we seek to minimize the distance between the two probability distributions $P$ and $Q_{\theta}$. But how do we even do this and why is there a min-max?

# Metrics and divergences

Let's quickly go through some definitions. In mathematics, we formalize the notion of distance through what is called a [metric](https://en.wikipedia.org/wiki/Metric_(mathematics)); this is a function that takes two elements from a set and outputs a non-negative number ($d: X \times X \rightarrow [0,\infty)$) with the following properties

1. $d(x,y) \geq 0$ (nonnegativity)
2. $d(x,y) = 0 \Longleftrightarrow x = y$ (identity of indiscernables)
3. $d(x,y) = d(y,x)$ (symmetry)
4. $d(x,z) \leq d(x,y) + d(y,z)$ (triangle inequality).

Divergence is a weaker notion of distance between probability distributions that only need to respect (1) and (2). We generally denote the divergence between $P$ and $Q$ as $D(P\vert \vert Q)$. Some well-known divergences are the [Kullbackâ€“Leibler distance (KL)](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence), the [Jensen-Shannon distance (JSD)](https://en.wikipedia.org/wiki/Jensen%E2%80%93Shannon_divergence), and the [Wasserstein distance](https://en.wikipedia.org/wiki/Wasserstein_metric). Some divergences like JSD and Wasserstein have symmetry (3) so $D(P\vert \vert Q)= D(Q\vert \vert P)$, but otherwise this is not the case.

# Divergence minimization

What we try to achieve with most statistical models is

$$
\min_\theta D(P \vert \vert Q_{\theta}).
$$

Note that there is no maximization, this is not a GAN yet. Here's how it gets more complicated... Divergences are generally defined in terms of the individual probabilities $p(x)$ and $q(y \vert \theta)$, for example, [f-divergences](https://en.wikipedia.org/wiki/F-divergence) (a class of divergences which include KL and JSD) are defined as

$$
D_{f}(P \vert \vert Q_{\theta}) = \int_{\Omega} p(x) \log\left(\frac{p(x)}{q(x \vert \theta)}\right)\,d\mu(x),
$$

where $\Omega$ is the sample space and $\mu$ is its associated measure. For simplicity, if $\Omega = \mathbb{R}$, then $d\mu(x)=dx$.

This is inherently problematic as we only have access to a finite amount of data samples and we do not know the distribution of the data ($p(x)$).

# Maximum Likelihood as a special case

The traditional approach is to take an approximation $\hat{P}$ of $P$ using the empirical distribution, i.e. we assume a discrete distribution where each data sample has probability $1/n$. In this case, [it can be shown](http://www.hongliangjie.com/2012/07/12/maximum-likelihood-as-minimize-kl-divergence/) that 

$$
\min_{\theta} D_{KL}(\hat{P} \vert \vert Q_{\theta}) = \max_{\theta} \log \mathcal{L}(\theta; x). 
$$ 

This means that maximum likelihood is just KL-divergence minimization with the empirical distribution approximation. I won't go through all the issues with maximum likelihood in generating realistic data, but as you can see, it's based on an approximation of the real data distribution and it assumes the use of the KL-divergence, different losses just imply a different $Q_{\theta}$ (e.g. normal distribution when using the least squares loss and binomial distribution when using the binary cross-entropy loss).

GANs work instead by solving a simpler "dual" version of the divergence that does not require knowing $p(x)$ nor $q(x \vert \theta)$. Therefore, we do not need to estimate the real data distribution using the empirical distribution nor are we forced to use KL-divergence. As you will see below, GANs only need the expectation $\mathbb{E}(X)$ with respect to $P$ and $Q_{\theta}$, which is easily estimated as an average over mini-batches!

Let me show you how it works for JSD ([Standard GAN](https://arxiv.org/abs/1406.2661)), f-divergence ([F-GAN](https://arxiv.org/abs/1606.00709); Standard GAN is a special case of this) and Wasserstein ([Wassertein GAN](https://arxiv.org/abs/1701.07875)).

# Jensen-Shannon distance

With JSD, [it can be shown](https://arxiv.org/abs/1406.2661) that

$$
\begin{align}
\max_{D:X \rightarrow \{0,1\}} V(G,D) &= \max_{D:X \rightarrow \{0,1\}} \mathbb{E}_{x \sim P}[\log(D(x))] + E_{x \sim Q_{\theta}}[1 - \log(D(x))] \\
&= \mathbb{E}_{x \sim P}\left[\log\left(\frac{p(x)}{p(x) + q(x \vert \theta)}\right)\right] + E_{x \sim Q_{\theta}}\left[1 - \log\left(\frac{q(x \vert \theta)}{p(x) + q(x \vert \theta)}\right)\right] \\
&= ... \\
&= -\log(4) + 2 JSD(P  \vert  \vert Q_{\theta}).
\end{align}
$$

This implies that

$$
\begin{align}
\min_\theta JSD(P  \vert  \vert Q_{\theta}) &= \frac{1}{2} \left(log(4) + \min_\theta \max_{D:X \rightarrow \{0,1\}} \mathbb{E}_{x \sim P}[\log(D(x))] + E_{x \sim Q_{\theta}}[1 - \log(D(x))] \right) \\
&\approx \frac{1}{2} \left(log(4) + \min_\theta \max_{w} \mathbb{E}_{x \sim P}[\log(D_w(x))] + E_{x \sim Q_{\theta}}[1 - \log(D_w(x))] \right).
\end{align}
$$

Thus, the Standard GAN loss minimizes the JSD (although the non-saturating version is slightly different as I mention later).

# F-divergence

We can obtain a dual version of a non-convex function through the [convex conjugate](https://en.wikipedia.org/wiki/Convex_conjugate) defined as

$$
f^*(t) = \sup_{u \in \text{dom}_f} \left \{ ut - f \left( u \right) \right \}.
$$

Using this equation in mind, [we can show](http://dept.stat.lsa.umich.edu/~xuanlong/Papers/Nguyen-Wainwright-Jordan-10.pdf) that

$$
\begin{align}
D_{f}(P  \vert  \vert  Q_{\theta}) &= \int_{\Omega} f\left(\frac{p(x)}{q(x \vert \theta)}\right) q(x \vert \theta)\,d\mu(x), \\
&= \int_{\Omega} \sup_{t \in \text{dom}_{f^*}} \left \{ t\frac{p(x)}{q(x \vert \theta)} - f^* \left( t \right) \right \} q(x \vert \theta)\,d\mu(x), \\
&= \sup_{T:X \rightarrow \text{dom}_{f^*}} \int_{\Omega} p(x)T(x)\,d\mu(x) - \int_{\Omega} q(x \vert \theta)f^*(T(x))\,d\mu(x) \\
&= \sup_{T:X \rightarrow \text{dom}_{f^*}} \mathbb{E}_{x \sim P}[T(x)] + E_{x \sim Q_{\theta}}[f^*(T(x))]
\end{align}
$$

This implies that

$$
\begin{align}
\min_\theta D_{f}(P  \vert  \vert  Q_{\theta}) &= \min_\theta \sup_{T:X \rightarrow \text{dom}_{f^*}} \mathbb{E}_{x \sim P}[T(x)] + E_{x \sim Q_{\theta}}[f^*(T(x))] \\
&\approx \min_\theta \max_{w} \mathbb{E}_{x \sim P}[T_w(x)] + E_{x \sim Q_{\theta}}[f^*(T_w(x))].
\end{align}
$$

Thus, the F-GAN loss minimizes f-divergences.

# Wassertein distance

With Wasserstein-1 distance, [it can be shown](https://www.math.hmc.edu/~su/papers.dir/metrics.pdf) that if $(\Omega, d)$ is a separable metric space ($d$ is a metric and $\Omega$ is the sample space as defined above) and $\int_{\Omega} d(x,y) \,d\mu(x) < \infty$, we have that

$$
\begin{alignat*}{2}
W_{1} (P  \vert  \vert  Q_{\theta}) &=\inf_{\gamma \in \Gamma (P, Q_{\theta})} \mathbb{E}_{(x,y) \sim \gamma} [d(x, y)] && (\text{Primal}) \\
&= \sup_{\substack{f:X \rightarrow \mathbb{R}\\ s.t.  \vert f(x)-f(y) \vert  \leq d(x,y)}} \mathbb{E}_{x \sim P}[f(x)] + E_{x \sim Q_{\theta}}[f(x)] \quad && (\text{Dual}),
\end{alignat*}
$$

where $\Gamma (P, Q_{\theta})$ is the set of all joint distributions with marginals $P$ and $Q_{\theta}$. Note that $\mathbb{R}^n$ is a separable space. Some other common versions of the theorem ask for a compact metric space instead. Although the space of images $256^{3 \times n}$ is compact, $\mathbb{R}^n$ is not compact, thus this version of the theorem is more general.

This implies that

$$
\begin{align}
\min_\theta  W_{1}(P  \vert  \vert  Q_{\theta}) &= \min_\theta \sup_{\substack{f:X \rightarrow \mathbb{R}\\ s.t.  \vert f(x)-f(y) \vert  \leq d(x,y)}} \mathbb{E}_{x \sim P}[f(x)] + E_{x \sim Q_{\theta}}[f(x)] \\
&\approx \min_\theta \max_{w} \mathbb{E}_{x \sim P}[f_w(x)] + E_{x \sim Q_{\theta}}[f_w(x)],
\end{align}
$$

assuming that $ \vert f_w(x)-f_w(y) \vert  \leq d(x,y)$ is respected for any $x$ and $y$ (very hard to impose though).

# Finding the optimal function

As shown above, common divergences can be reformulated as a maximum/supremum of some function involving expectations with respect to the two probability distributions $P$ and $Q_{\theta}$. We seek to find the "discriminator function" ($D(x)$ with GAN, $T(x)$ with F-GAN and $f(x)$ with Wasserstein) that maximize this formula. This is a bit different from standard min/max problems where we seek to find the optimal weights (Generally in $\mathbb{R}$ for gradient methods and in binary for integer programming) that leads to the minimum. Here, we seek instead to find the optimal function out of all possible functions of a certain type (With standard GAN it's all functions that take the data and return a binary output). We estimate the discriminator $f(x)$ as $\hat{f}(x \vert w)$ using a neural network so that we can estimate the weights of a very general function through gradient descent rather than searching for the best function (an impossible task really). Our hope is that given the very large capacity of neural networks, we will reach something close to the supremum. This means that there will be a gap between the estimated divergence and the true divergence, but hopefully, it should be small. 

# What is done in the litterature

Given this knowledge, many researchers have started working on divergences already defined as maximization problems over expectations of functions rather than defining a primal and finding it's "dual". Many researchers focus on [Integral probability metrics](https://arxiv.org/abs/0901.2698) (IPM)

$$
IPM_{F} (P  \vert  \vert  Q_{\theta}) = \sup_{f \in F} \mathbb{E}_{x \sim P}[f(x)] + E_{x \sim Q_{\theta}}[f(x)],
$$

where $F$ is an arbitrary set of functions chosen a priori. Note that although IPMs form a very broad class of divergences, they represent just some of the many possible divergences we can consider. Some even brought the variance ($\mathbb{E}_[f(x)^2]$) into the equation (e.g. [Fisher GAN](https://arxiv.org/abs/1705.09675) by Mroueh et  Sercu (2017)). It's still unclear which divergence (or class of function $F$ for the IPMs) one should use to reach the highest quality of generated data and ease of optimization. 

What's particularly interesting is that the current approaches which lead to the best quality in image generation are [progressive](https://arxiv.org/abs/1710.10196) (Karras et al. (2018)) or [pyramidal](https://arxiv.org/abs/1710.10916) (Zhang et al. (2017)) meaning that they require many networks trained in succession over a long period of time using complex neural engineering. One would hope that we could find a GAN loss that could instead directly train a single model from zero to optimal quality. This is an ongoing area of research that is very interesting!

# Problems and remaining mysteries

Given that some divergences have simple form that do not require the use of the "dual" (e.g. [Maximum mean matching GAN](https://arxiv.org/abs/1505.03906) by GK Dziugaite et al. (2015)), it is still not clear why min-max optimizations so far tend to work better than pure minimizations, but there is certainly no reason to think that this will always be the case.

Note that the usual problems like under/overfitting apply to GANs, but they are nastier and cause a lot more issues. Although the discriminator may appear to lead to the supremum on observed data, it might fail on unobserved data, thus be overfitting. The fact that the discriminator may under/overfit is a massive problem because if this happens, the generator will not be minimizing the right distance at all which will prevent it from learning. This is part of what makes GANs so hard to train.

A mystery that remains is that Standard GAN works best when the generator minimizes a slightly different term, i.e. $-E_{x \sim Q_{\theta}}[\log(D(x))]$ instead of $E_{x \sim Q_{\theta}}[1 - \log(D(x))]$. We call this variant the non-saturating loss. The authors of the F-GANs have made a similar observation. Note that both saturating and non-saturating losses lead to the same fixed point dynamics. Although there are numerical/intuitive reasons why the saturating loss might be better, it is not perfectly clear mathematically why it is so much better and what divergence the generator would be minimizing. This is why some papers are suggesting that we might [not be minimizing a divergence at every step](https://arxiv.org/abs/1710.08446) (see Fedus et al. (2017)) and that there might be some hidden mathematical phenomenon that we don't know about yet. As far as I know, this issue has only been observed with f-divergences so it might be an isolated case. 

Note that [Arjovsky et al. (2017)](https://arxiv.org/abs/1701.07875) showed that f-divergences should not converge at all if the support of $P$ and $Q_{theta}$ is different (i.e. if for some data $x$, $p(x) = 0$ but $q(x \vert \theta) \neq 0$, or vice-versa). This does not explain why the non-saturating loss works well, but it may explain why the saturating loss works so poorly.

# Summary / Conclusion

In conclusion, the discriminator purpose is to estimate the divergence while the generator purpose is to minimize that estimated divergence. So from that general divergence point-of-view, it is not really an adversarial game between two competitors and the generator is not really trying to "fool the discriminator". However, GANs can still be seen as a min-max game where we try to reach an equilibrium/saddle-point such that neither $G$ or $D$ can further improve.

I hope this was helpful for some of you! :D

For an excellent summary of the divergences represented as maximization problems over a class of functions that we use in GANs, I highly recommend taking a look at Table 1 in page 4 of the [Sobolev GAN paper](https://openreview.net/forum?id=SJA7xfb0b).

Notes:
* I often use brackets when I say "dual" because for f-divergences it's not really the dual of the divergence, but the dual of a function inside the divergence.
* I generally assume that the supremum is the same thing as the maximum because in practice we do not work with open sets. Just to give you an example of the supremum being different from the maximum: the set $(-\infty, 0)$ has no maximum although it has a supremum at 0.