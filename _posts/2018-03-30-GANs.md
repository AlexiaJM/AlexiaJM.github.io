---
layout: post
title: Understanding GANs on a fundamental level through a statistical divergence perspective
published: true
---

Most introductions to Generative adversarial networks (GAN) focus on a game-theoretic perspective with two players where one player (the discriminator) tries to differentiate the real data from the fake data while the other player (the generator) learns to generate realistic data by trying to "fool" the discriminator. However, this view of GANs is very limited as it does not generalize to GANs other than one initially formulated by Goodfellow et al. (2014). 

After reading a lot on the subject through many dense papers, I started making sense of what GANs are fundamentally. Most researchers specialized in GANs already know this but the majority of ML-enthusiasts probably don't and some GAN papers suggest that a few authors don't understand this well so I thought it would be really useful to lay out a short summary of this perspective on the subject.

Let start from a simple problem, let say we want to minimize the distance between an object in $\mathbb{R}^n$ and an object generated by a neural network (assuming a fixed input so that the neural network always generate the same thing). The solution is to minimize some sort of distance between the two objects until we reach convergence. A slightly harder problem would be to minimize the distance between a set of objects and another set of objects generated by a neural network, however, this would make the assumption that all our real data is the only data possible (exhausting cases). In GANs we assume that the real data comes from an underlying probability distribution $P$ and that the data generated by the generator comes from an underlying probability distribution $Q_{\theta}$ (produced by a neural network); we seek to minimize the distance between the two probability distributions $P$ and $Q_{\theta}$. But how do we even do this and why is there a min-max?

Let's quickly go through some definitions. In mathematics, we formalize the notion of distance through what is called a *metric*; this is a function that takes two elements from a set and outputs a non-negative number ($d: X \times X \rightarrow [0,\infty)$) with the following properties:

1. $d(x,y) \geq 0$ (nonnegativity)
2. $d(x,y) = 0 \Longleftrightarrow x = y$ (identity of indiscernables)
3. $d(x,y) = d(y,x)$ (symmetry)
4. $d(x,z) \leq d(x,y) + d(y,z)$ (triangle inequality).

Divergence is a weaker notion of distance between probability distributions that only need to respect (1) and (2). We generally denote the divergence between $P$ and $Q$ as $D(P\vert \vert Q)$. Some well-known divergences are the Kullbackâ€“Leibler distance (KL), the Jensen-Shannon distance (JSD), and the Wasserstein distance. Some divergences like JSD and Wasserstein have symmetry (3) so $D(P\vert \vert Q)= D(Q\vert \vert P)$, but otherwise this is not the case.

What we try to achieve with most statistical models is:

$$
\min_\theta D(P \vert \vert Q_{\theta}).
$$

Note that there is no maximization, this is not a GAN yet. Here's how it gets more complicated... Divergences are generally defined in terms of the individual probabilities $p(x)$ and $p(y)$, for example f-divergences (a class of divergences which include KL and JSD) are defined as

$$
D_{f}(P \vert \vert Q_{\theta}) = \int_{\Omega} p(x) \log\left(\frac{p(x)}{q(x \vert \theta)}\right)\,d\mu(x),
$$

where $\Omega$ is the sample space and $\mu$ is its associated measure. For simplicity, if $\Omega = \mathbb{R}$, then $d\mu(x)=dx$.

This is inherently problematic as we only have access to a finite amount of data samples, we do not know the distribution of the data ($p(x)$). The traditional approach is to take an approximation $\hat{P}$ of $P$ using the empirical distribution, i.e. we assume a discrete distribution where each data sample has probability $1/n$. In this case, it can be shown that 

$$
\min_{\theta} D_{KL}(\hat{P} \vert \vert Q_{\theta}) = \max_{\theta} \log \mathcal{L}(\theta; x). 
$$ 

This means that maximum likelihood is just KL-divergence minimization with the empirical distribution approximation. I won't go through all the issues with maximum likelihood in generating realistic data but as you can see, it's based on an approximation of the real data distribution and it assumes the use of the KL-divergence, different losses just imply a different $Q_{\theta}$ (e.g. least squares loss is normal distribution and binary cross-entropy loss is binomial distribution).

GANs work instead by solving a simpler "dual" version of the divergence that does not require knowing $p(x)$ nor $q(x \vert \theta)$. Therefore we do not need to estimate the real data distribution using the empirical distribution nor are we forced to use KL-divergence. As you will see below, GANs only need the expectation $\mathbb{E}(X)$ with respect to $P$ and $Q_{\theta}$ which is easily estimated as an average over mini-batches!

Let me show you how it works for JSD (Standard GAN), f-divergence (F-GAN; Standard GAN is a special case of this) and Wasserstein.

# Jensen-Shannon distance

With JSD, it can be shown (see Goodfellow et al. (2014)) that

$$
\begin{align}
\max_{D:X \rightarrow \{0,1\}} V(G,D) &= \max_{D:X \rightarrow \{0,1\}} \mathbb{E}_{x \sim P}[\log(D(x))] + E_{x \sim Q_{\theta}}[1 - \log(D(x))] \\
&= \mathbb{E}_{x \sim P}\left[\log\left(\frac{p(x)}{p(x) + q(x \vert \theta)}\right)\right] + E_{x \sim Q_{\theta}}\left[1 - \log\left(\frac{q(x \vert \theta)}{p(x) + q(x \vert \theta)}\right)\right] \\
&= ... \\
&= -\log(4) + 2 JSD(P  \vert  \vert Q_{\theta})
\end{align}
$$

This implies that

$$
\begin{align}
\min_\theta JSD(P  \vert  \vert Q_{\theta}) &= \frac{1}{2} \left(log(4) + \min_\theta \max_{D:X \rightarrow \{0,1\}} \mathbb{E}_{x \sim P}[\log(D(x))] + E_{x \sim Q_{\theta}}[1 - \log(D(x))] \right) \\
&\approx \frac{1}{2} \left(log(4) + \min_\theta \max_{w} \mathbb{E}_{x \sim P}[\log(D_w(x))] + E_{x \sim Q_{\theta}}[1 - \log(D_w(x))] \right).
\end{align}
$$

Thus, the Standard GAN loss minimize the JSD (althought the non-saturing version is slightly different as I mention later).

# F-divergence

We can obtain a dual version of a non-convex function through the convex conjugate defined as
$$
f^*(t) = \sup_{u \in \text{dom}_f} \left \{ ut - f \left( u \right) \right \}.
$$
Using this equation in mind, we can show (see Nowozin et al. (2016)) that
$$
\begin{align}
D_{f}(P || Q_{\theta}) &= \int_{\Omega} f\left(\frac{p(x)}{q(x|\theta)}\right) q(x|\theta)\,d\mu(x), \\
&= \int_{\Omega} \sup_{t \in \text{dom}_{f^*}} \left \{ t\frac{p(x)}{q(x|\theta)} - f^* \left( t \right) \right \} q(x|\theta)\,d\mu(x), \\
&= \sup_{T:X \rightarrow \text{dom}_{f^*}} \int_{\Omega} p(x)T(x)\,d\mu(x) - \int_{\Omega} q(x|\theta)f^*(T(x))\,d\mu(x) \\
&= \sup_{T:X \rightarrow \text{dom}_{f^*}} \mathbb{E}_{x \sim P}[T(x)] + E_{x \sim Q_{\theta}}[f^*(T(x))]
\end{align}
$$
This implies that
$$
\begin{align}
\min_\theta D_{f}(P || Q_{\theta}) &= \min_\theta \sup_{T:X \rightarrow \text{dom}_{f^*}} \mathbb{E}_{x \sim P}[T(x)] + E_{x \sim Q_{\theta}}[f^*(T(x))] \\
&\approx \min_\theta \max_{w} \mathbb{E}_{x \sim P}[T_w(x)] + E_{x \sim Q_{\theta}}[f^*(T_w(x))].
\end{align}
$$
Thus, F-GAN loss minimize f-divergences.

# Wassertein distance

With Wasserstein-1 distance, it can be shown that if $(\Omega, d)$ is a seperable metric space ($d$ is a metric and $\Omega$ is the sample space as defined above) and
$$
\int_{\Omega} d(x,y) \,d\mu(x) < \infty ,
$$ 
we have that
$$
\begin{align}
W_{1} (P || Q_{\theta}) &=\inf_{\gamma \in \Gamma (P, Q_{\theta})} \mathbb{E}_{(x,y) \sim \gamma} [d(x, y)] \quad (\text{Primal}) \\
&= \sup_{\substack{f:X \rightarrow \mathbb{R}\\ s.t. |f(x)-f(y)| \leq d(x,y)}} \mathbb{E}_{x \sim P}[f(x)] + E_{x \sim Q_{\theta}}[f(x)] \quad (\text{Dual}),
\end{align}
$$
where $\Gamma (P, Q_{\theta})$ is the set of all joint distributions with marginals $P$ and $Q_{\theta}$. Note that $\mathbb{R}^n$ is a seperable space. Some other common versions of the theorem ask for a compact metric space instead. Although the space of images $256^{3 \times n}$ is compact, $\mathbb{R}^n$ is not compact thus this version of the theorem is more general.

This implies that
$$
\begin{align}
\min_\theta  W_{1}(P || Q_{\theta}) &= \min_\theta \sup_{\substack{f:X \rightarrow \mathbb{R}\\ s.t. |f(x)-f(y)| \leq d(x,y)}} \mathbb{E}_{x \sim P}[f(x)] + E_{x \sim Q_{\theta}}[f(x)] \\
&\approx \min_\theta \max_{w} \mathbb{E}_{x \sim P}[f_w(x)] + E_{x \sim Q_{\theta}}[f_w(x)],
\end{align}
$$
assuming that $|f_w(x)-f_w(y)| \leq d(x,y)$ is respected for any $x$ and $y$ (very hard to impose though).

--

As shown above, common divergences can be reformulated as a maximum/supremum of some function involving expectations with respect to the two probability distributions $P$ and $Q_{\theta}$. We seek to find the "discriminator function" ($D(x)$ with GAN, $T(x)$ with F-GAN and $f(x)$ with Wasserstein) that maximize this. This is what GANs are all about.

It is a bit different from standard min/max problems where we seek to find the optimal weights (Generally in $\mathbb{R}$ for gradients methods and in $\{0,1\}$ for integer programming optimizations) that leads to the minimum. Here we seek instead to find the optimal function out of all possible functions of a certain type (With standard GAN it's all functions that take the data and return a binary output). We estimate the discriminator $f(x)$ as $\hat{f}(x|w)$ using a neural network so that can estimate the weights of a very general function trough gradient descent rather than searching for the best function (impossible task really). Our hope is that given the very large capacity of neural networks, we will reach something close to the supremum. This means that there will be a gap between the estimated divergence and the true divergence but hopefully, it should be small. 

Given this knowledge, many researchers have started working on divergences already defined as maximization problems over functions rather than defining a primal and finding it's "dual". Some even started including the variance in such formula (e.g. Fisher GAN by Mroueh et  Sercu (2017)). Many researchers focus on Integral probability metrics (IPM)
$$
IPM_{F} (P || Q_{\theta}) = \sup_{f \in F} \mathbb{E}_{x \sim P}[f(x)] + E_{x \sim Q_{\theta}}[f(x)],
$$
where $F$ is an arbitrary set of functions chosen a priori. Note that although IPMs form a very broad class of divergences, they represent just some of the many possible divergences we can consider. It's still unclear which divergence (or class of function $F$ for the IPMs) to choose to reach the highest quality of generated data and ease of optimization. 

What's particularly interesting is that the current approaches that lead to the best quality in image generation are progressive (Karras et al. (2018)) or pyramidal (Zhang et al. (2017)) meaning that they require many networks trained in succession trained over a long period of time using complex neural engineering. One would hope that we could find a GAN loss that could instead directly train a single model from zero to optimal quality. This is an ongoing area of research that is very interesting!

Given that some divergences have simple forms that do not require the use of the "dual" (e.g. Maximum mean matching GAN by GK Dziugaite et al. (2015)), it is still not clear why min-max optimizations so far tend to work better but there is certainly no reason to think that this will always be the case.

Note that the usual problems like under/overfitting apply to GANs but they are much more nasty and cause a lot more issues. Although the discriminator may appear to lead to the supremum on observed data, it might fail on unobserved data and thus be overfitting. The fact that the discriminator may under/overfit is a massive problem because if this happens, the generator will not be minimizing the right distance at all which will prevent the generator from learning. This is part of what makes GANs so hard to train.

A mystery that remains is that Standard GAN work best when the generator minimize a slightly different term, i.e. $-E_{x \sim Q_{\theta}}[\log(D(x))]$ instead of $E_{x \sim Q_{\theta}}[1 - \log(D(x))]$. We have a similar observation with F-GANs.  Note that we still reach the same fixed point dynamics using this alternative loss. Although there are numerical/intuitive reasons why this might be better, it is not perfectly clear mathematically why it is so much better and what divergence the generator would we be minimizing. Which is why some papers are suggesting that we are not necessarily minimizing a divergence at every step (see Fedus et al. (2017)) and that there might be some hidden mathematical phenomenon that we don't know about yet. As far as I know, this issue has only been observed with f-divergences. Note that Arjovsky et al. (2017) showed that f-divergences should not converge at all mathematically if the support of $P$ and $Q_{theta}$ is different (i.e. if for some data $x$, $p(x) = 0$ but $q(x|\theta) \neq 0$, or vice-versa). This does not explain why the non-saturating loss works well but it may explain why the original loss works so poorly that we need to modify it for things to work.

In conclusion, the discriminator purpose is to estimate the divergence while the generator purpose is to minimize that estimated divergence. So from that general point-of-view, it is not really an adversarial game between two competitors and the generator is not really trying to "fool the discriminator". However, GANs can still be seen as a min-max game where we try to reach an equilibrium/saddle-point such that neither $G$ or $D$ can further improve.

I hope this was helpful for some of you! :D

Notes:
* I often use brackets when I say "dual" because for f-divergences it's not really the dual of the divergence but the dual of a function inside the divergence
* I generally assume that the supremum is the same thing as the maximum because in practice we do not work with open sets. Just to give you an example of the supremum being different from the maximum, the set $(-\infty, 0)$ has no maximum although it has a supremum at 0.