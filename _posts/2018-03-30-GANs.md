---
layout: post
title: Understanding GANs on a fundamental level through a statistical divergence perspective
published: true
---

Most introductions to Generative adversarial networks (GAN) focus on a game-theoretic perspective with two players where one player (the discriminator) tries to differentiate the real data from the fake data while the other player (the generator) learns to generate realistic data by trying to "fool" the discriminator. However, this view of GANs is very limited as it does not generalize to GANs other than one initially formulated by Goodfellow et al. (2014). 

After reading a lot on the subject through many dense papers, I started making sense of what GANs are fundamentally. Most researchers specialized in GANs already know this but the majority of ML-enthusiasts probably don't and some GAN papers suggest that a few authors don't understand this well so I thought it would be really useful to lay out a short summary of this perspective on the subject.

Let start from a simple problem, let say we want to minimize the distance between an object in $\mathbb{R}^n$ and an object generated by a neural network (assuming a fixed input so that the neural network always generate the same thing). The solution is to minimize some sort of distance between the two objects until we reach convergence. A slightly harder problem would be to minimize the distance between a set of objects and another set of objects generated by a neural network, however, this would make the assumption that all our real data is the only data possible (exhausting cases). In GANs we assume that the real data comes from an underlying probability distribution $P$ and that the data generated by the generator comes from an underlying probability distribution $Q_{\theta}$ (produced by a neural network); we seek to minimize the distance between the two probability distributions $P$ and $Q_{\theta}$. But how do we even do this and why is there a min-max?

Let's quickly go through some definitions. In mathematics, we formalize the notion of distance through what is called a *metric*; this is a function that takes two elements from a set and outputs a non-negative number ($d: X \times X \rightarrow [0,\infty)$) with the following properties:

1. $d(x,y) \geq 0$ (nonnegativity)
2. $d(x,y) = 0 \Longleftrightarrow x = y$ (identity of indiscernables)
3. $d(x,y) = d(y,x)$ (symmetry)
4. $d(x,z) \leq d(x,y) + d(y,z)$ (triangle inequality).

Divergence is a weaker notion of distance between probability distributions that only need to respect (1) and (2). We generally denote the divergence between $P$ and $Q$ as $D(P\vert \vertQ)$. Some well-known divergences are the Kullbackâ€“Leibler distance (KL), the Jensen-Shannon distance (JSD), and the Wasserstein distance. Some divergences like JSD and Wasserstein have symmetry (3) so $D(P\vert \vertQ)= D(Q\vert \vertP)$, but otherwise this is not the case.

What we try to achieve with most statistical models is:
$$\min_\theta D(P \vert \vert Q_{\theta}).$$
Note that there is no maximization, this is not a GAN yet. Here's how it gets more complicated... Divergences are generally defined in terms of the individual probabilities $p(x)$ and $p(y)$, for example f-divergences (a class of divergences which include KL and JSD) are defined as
$$
D_{f}(P \vert \vert Q_{\theta}) = \int_{\Omega} p(x) \log\left(\frac{p(x)}{q(x \vert \theta)}\right)\,d\mu(x),
$$
where $\Omega$ is the sample space and $\mu$ is its associated measure. For simplicity, if $\Omega = \mathbb{R}$, then $d\mu(x)=dx$.

This is inherently problematic as we only have access to a finite amount of data samples, we do not know the distribution of the data ($p(x)$). The traditional approach is to take an approximation $\hat{P}$ of $P$ using the empirical distribution, i.e. we assume a discrete distribution where each data sample has probability $1/n$. In this case, it can be shown that $$ \min_{\theta} D_{KL}(\hat{P} \vert \vert Q_{\theta}) = \max_{\theta} \log \mathcal{L}(\theta; x). $$ 
This means that maximum likelihood is just KL-divergence minimization with the empirical distribution approximation. I won't go through all the issues with maximum likelihood in generating realistic data but as you can see, it's based on an approximation of the real data distribution and it assumes the use of the KL-divergence, different losses just imply a different $Q_{\theta}$ (e.g. least squares loss is normal distribution and binary cross-entropy loss is binomial distribution).

GANs work instead by solving a simpler "dual" version of the divergence that does not require knowing $p(x)$ nor $q(x \vert \theta)$. Therefore we do not need to estimate the real data distribution using the empirical distribution nor are we forced to use KL-divergence. As you will see below, GANs only need the expectation $\mathbb{E}(X)$ with respect to $P$ and $Q_{\theta}$ which is easily estimated as an average over mini-batches!

Let me show you how it works for JSD (Standard GAN), f-divergence (F-GAN; Standard GAN is a special case of this) and Wasserstein.

*Jensen-Shannon distance*

With JSD, it can be shown (see Goodfellow et al. (2014)) that

$$
\begin{align}
\max_{D:X \rightarrow \{0,1\}} V(G,D) &= \max_{D:X \rightarrow \{0,1\}} \mathbb{E}_{x \sim P}[\log(D(x))] + E_{x \sim Q_{\theta}}[1 - \log(D(x))] \\
&= \mathbb{E}_{x \sim P}\left[\log\left(\frac{p(x)}{p(x) + q(x \vert \theta)}\right)\right] + E_{x \sim Q_{\theta}}\left[1 - \log\left(\frac{q(x \vert \theta)}{p(x) + q(x \vert \theta)}\right)\right] \\
&= ... \\
&= -\log(4) + 2 JSD(P  \vert  \vert Q_{\theta})
\end{align*}
This implies that
\begin{align*}
\min_\theta JSD(P  \vert  \vert Q_{\theta}) &= \frac{1}{2} \left(log(4) + \min_\theta \max_{D:X \rightarrow \{0,1\}} \mathbb{E}_{x \sim P}[\log(D(x))] + E_{x \sim Q_{\theta}}[1 - \log(D(x))] \right) \\
&\approx \frac{1}{2} \left(log(4) + \min_\theta \max_{w} \mathbb{E}_{x \sim P}[\log(D_w(x))] + E_{x \sim Q_{\theta}}[1 - \log(D_w(x))] \right).
\end{align}
$$
Thus, the Standard GAN loss minimize the JSD (althought the non-saturing version is slightly different as I mention later).